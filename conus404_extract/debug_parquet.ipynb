{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T22:24:08.183540Z",
     "iopub.status.busy": "2024-11-28T22:24:08.183321Z",
     "iopub.status.idle": "2024-11-28T22:24:08.913543Z",
     "shell.execute_reply": "2024-11-28T22:24:08.913071Z",
     "shell.execute_reply.started": "2024-11-28T22:24:08.183523Z"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7878a77d059eb84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T22:24:08.914503Z",
     "iopub.status.busy": "2024-11-28T22:24:08.914042Z",
     "iopub.status.idle": "2024-11-28T22:24:08.917302Z",
     "shell.execute_reply": "2024-11-28T22:24:08.916968Z",
     "shell.execute_reply.started": "2024-11-28T22:24:08.914486Z"
    }
   },
   "outputs": [],
   "source": [
    "r = '/caldera/hovenweep/projects/usgs/water'\n",
    "d = os.path.join(r, 'wymtwsc', 'dketchum')\n",
    "\n",
    "if not os.path.isdir(d):\n",
    "    home = os.path.expanduser('~')\n",
    "    d = os.path.join(home, 'data', 'IrrigationGIS')\n",
    "\n",
    "c404 = os.path.join(d, 'conus404')\n",
    "dads = os.path.join(d, 'dads')\n",
    "ghcn = os.path.join(d, 'climate', 'ghcn')\n",
    "\n",
    "sites = os.path.join(dads, 'met', 'stations', 'madis_29OCT2024.csv')\n",
    "\n",
    "model_target = 'ba'\n",
    "if model_target == 'ba':\n",
    "    csv_files = os.path.join(c404, 'station_data_ba')\n",
    "    p_files = os.path.join(c404, 'parquet_ba')\n",
    "else:\n",
    "    csv_files = os.path.join(c404, 'station_data')\n",
    "    p_files = os.path.join(c404, 'parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2205f6c4d297eac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T22:24:10.655833Z",
     "iopub.status.busy": "2024-11-28T22:24:10.655469Z",
     "iopub.status.idle": "2024-11-28T22:24:10.658297Z",
     "shell.execute_reply": "2024-11-28T22:24:10.657965Z",
     "shell.execute_reply.started": "2024-11-28T22:24:10.655804Z"
    }
   },
   "outputs": [],
   "source": [
    "stations = sites\n",
    "root = csv_files\n",
    "outdir = p_files\n",
    "workers = 1\n",
    "debug = True\n",
    "missing_file = None\n",
    "start_date='2000-01-01'\n",
    "end_date='2022-09-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f3b53980cb32b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T22:24:11.519765Z",
     "iopub.status.busy": "2024-11-28T22:24:11.519410Z",
     "iopub.status.idle": "2024-11-28T22:24:11.573341Z",
     "shell.execute_reply": "2024-11-28T22:24:11.572940Z",
     "shell.execute_reply.started": "2024-11-28T22:24:11.519735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 stations to write\n",
      "sample stations for the selected region:\n",
      "          fid   latitude   longitude         elev      stype\n",
      "fid                                                        \n",
      "TR266  TR266  47.045933 -114.112076  1353.599976       RAWS\n",
      "MTNNM  MTNNM  47.022991 -114.388680   910.700012   utmesnet\n",
      "G2344  G2344  46.903332 -114.115669   961.030029  APRSWXNET\n",
      "AV610  AV610  46.526669 -114.047829  1164.000000  APRSWXNET\n",
      "FINM8  FINM8  47.045929 -114.112068  1353.599976       RAWS\n"
     ]
    }
   ],
   "source": [
    "start = pd.to_datetime(start_date)\n",
    "end = pd.to_datetime(end_date)\n",
    "\n",
    "required_months = pd.date_range(start=start, end=end, freq='MS').strftime('%Y%m').tolist()\n",
    "expected_index = pd.date_range(start=start, end=end, freq='h')\n",
    "\n",
    "strdt = [d.strftime('%Y%m%d%H') for d in expected_index]\n",
    "\n",
    "station_list = pd.read_csv(stations)\n",
    "if 'LAT' in station_list.columns:\n",
    "    station_list = station_list.rename(columns={'STAID': 'fid', 'LAT': 'latitude', 'LON': 'longitude'})\n",
    "w, s, e, n = (-125.0, 25.0, -67.0, 53.0)\n",
    "station_list = station_list[(station_list['latitude'] < n) & (station_list['latitude'] >= s)]\n",
    "station_list = station_list[(station_list['longitude'] < e) & (station_list['longitude'] >= w)]\n",
    "\n",
    "station_list = station_list.sample(frac=1)\n",
    "subdirs = station_list['fid'].to_list()\n",
    "# subdirs.sort()\n",
    "\n",
    "print(f'{len(subdirs)} directories to check')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a07bc3-75ee-4144-8ab6-b263df63dd47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T22:24:12.496142Z",
     "iopub.status.busy": "2024-11-28T22:24:12.495713Z",
     "iopub.status.idle": "2024-11-28T22:24:12.512895Z",
     "shell.execute_reply": "2024-11-28T22:24:12.512558Z",
     "shell.execute_reply.started": "2024-11-28T22:24:12.496124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1251436.5323012709,\n",
       " 929060.7052309539,\n",
       " -1157553.5378020902,\n",
       " 1023945.1012115715)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conus404_parquet(root_, subdir_, required_months_, expected_index_, strdt_, outdir_, write_missing=None):\n",
    "    subdir_path = os.path.join(root_, subdir_)\n",
    "    out_file = os.path.join(outdir_, f'{subdir_}.parquet.gzip')\n",
    "\n",
    "    if os.path.isdir(subdir_path):\n",
    "\n",
    "        csv_files_ = [f for f in os.listdir(subdir_path) if f.endswith('.csv')]\n",
    "\n",
    "        if os.path.exists(out_file) and csv_files_:\n",
    "            shutil.rmtree(subdir_path)\n",
    "            print(f'{os.path.basename(out_file)} exists, removing {len(csv_files)} csv files')\n",
    "            return\n",
    "\n",
    "        dtimes = [f.split('_')[-1].replace('.csv', '') for f in csv_files_]\n",
    "        rm_files = csv_files_.copy()\n",
    "\n",
    "        if len(dtimes) < len(required_months_):\n",
    "            missing = [m for m in required_months_ if m not in dtimes]\n",
    "            if len(missing) > 0:\n",
    "                print(f'{subdir_} missing {len(missing)} months: {np.random.choice(missing, size=5, replace=False)}')\n",
    "                return\n",
    "\n",
    "        dfs = []\n",
    "        for file in csv_files_:\n",
    "            c = pd.read_csv(os.path.join(subdir_path, file), parse_dates=['dt'],\n",
    "                            date_format='%Y%m%d%H')\n",
    "            dfs.append(c)\n",
    "        df = pd.concat(dfs)\n",
    "        df = df.drop_duplicates(subset='dt', keep='first')\n",
    "        df = df.set_index('dt').sort_index()\n",
    "        df = df.drop(columns=['fid', 'time_bnds'])\n",
    "\n",
    "        missing = len(expected_index_) - df.shape[0]\n",
    "        if missing > 15:\n",
    "            counts, missing_list = {}, []\n",
    "            missing_idx = [i for i in expected_index_ if i not in df.index]\n",
    "            for midx in missing_idx:\n",
    "                dt = f'{midx.year}{midx.month:02}'\n",
    "                if dt not in counts.keys():\n",
    "                    counts[dt] = 1\n",
    "                else:\n",
    "                    counts[dt] += 1\n",
    "                p = f'NLDAS_FORA0125_H.A{midx.year}{midx.month:02}{midx.day:02}.{midx.hour}00.020.nc'\n",
    "                f = os.path.join('/data/ssd1/nldas2/netcdf', p)\n",
    "                if os.path.exists(f):\n",
    "                    missing_list.append(1)\n",
    "\n",
    "            print(f'{subdir_} is missing {missing} rows')\n",
    "            # [print(k, v) for k, v in counts.items()]\n",
    "\n",
    "            counts = {k: v for k, v in counts.items() if v > 1}\n",
    "\n",
    "            if write_missing:\n",
    "                with open(write_missing, 'w') as fp:\n",
    "                    json.dump({'missing': list(counts.keys())}, fp, indent=4)\n",
    "                print(f'wrote missing dates to {write_missing}, exiting')\n",
    "                exit()\n",
    "            return\n",
    "\n",
    "        elif missing > 0:\n",
    "            df = df.reindex(expected_index_)\n",
    "            df = df.interpolate(method='linear')\n",
    "\n",
    "        df['dt'] = strdt_\n",
    "\n",
    "        df.to_parquet(out_file, compression='gzip')\n",
    "        shutil.rmtree(subdir_path)\n",
    "        print(f'wrote {subdir_}, removed {len(rm_files)} .csv files,'\n",
    "              f' {datetime.strftime(datetime.now(), '%Y%m%d %H:%M')}')\n",
    "        return\n",
    "    else:\n",
    "        if os.path.exists(out_file):\n",
    "            print(f'{os.path.basename(out_file)} exists, skipping')\n",
    "        else:\n",
    "            print(f'{subdir_} not found')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639d96ad-b66a-40d5-b19c-dbc6f0c83cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T22:24:14.375141Z",
     "iopub.status.busy": "2024-11-28T22:24:14.374729Z",
     "iopub.status.idle": "2024-11-28T22:24:15.841323Z",
     "shell.execute_reply": "2024-11-28T22:24:15.840783Z",
     "shell.execute_reply.started": "2024-11-28T22:24:14.375110Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dketchum/miniconda3/envs/hyt/lib/python3.10/site-packages/intake_xarray/base.py:21: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  'dims': dict(self._ds.dims),\n"
     ]
    }
   ],
   "source": [
    "output_mode = 'ba'\n",
    "hytest_cat = intake.open_catalog(\n",
    "    \"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\n",
    "cat = hytest_cat['conus404-catalog']\n",
    "if output_mode == 'uncorrected':\n",
    "    # model output, uncorrected\n",
    "    dataset = 'conus404-hourly-onprem-hw'\n",
    "    out_data = csv_files = os.path.join(c404, 'station_data')\n",
    "elif output_mode == 'ba':\n",
    "    # bias-adjusted for precip and temp\n",
    "    dataset = 'conus404-hourly-ba-onprem-hw'\n",
    "    out_data = csv_files = os.path.join(c404, 'station_data_ba')\n",
    "else:\n",
    "    raise ValueError('output_mode not recognized')\n",
    "ds = cat[dataset].to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18acd2e838f671b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T22:24:17.208957Z",
     "iopub.status.busy": "2024-11-28T22:24:17.208394Z",
     "iopub.status.idle": "2024-11-28T22:24:17.219092Z",
     "shell.execute_reply": "2024-11-28T22:24:17.218628Z",
     "shell.execute_reply.started": "2024-11-28T22:24:17.208932Z"
    }
   },
   "outputs": [],
   "source": [
    "if missing_file:\n",
    "    for sd in subdirs:\n",
    "        conus404_parquet(root, sd, required_months, expected_index, strdt, outdir, missing_file)\n",
    "\n",
    "if debug:\n",
    "    for subdir in subdirs:\n",
    "        conus404_parquet(root, subdir, required_years, expected_index, strdt, outdir)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "    executor.map(conus404_parquet, [root] * len(subdirs), subdirs,\n",
    "                 [required_years] * len(subdirs),\n",
    "                 [expected_index] * len(subdirs), [strdt] * len(subdirs),\n",
    "                 [outdir] * len(subdirs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyt",
   "language": "python",
   "name": "hyt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
